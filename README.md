# poker-goat by Zane Bookbinder and Rahul Dasgupta (along with Professor Dave Byrd)

### Overview
This repository contains the code for Reinforcement Learning with a Python-based 
game of Texas Hold'em, simplified to eliminate the turn and the river (only 
three common cards). It was created for our independent study during the Fall
of 2023 with Professor Dave Byrd at Bowdoin College.

### What We Did
We attempted to use Reinforcement Learning to create a Poker player that could
understand its cards, the game situation, etc. and then make an optimal decision
about what action to take. We began by building the game, then began creating 
RL experiences from game play. After that, we incorportated a model that chose 
actions. We calculated the correct Q values using the Bellman Equation. Rewards
corresponded to chip win/loss amounts, and actions that didn't end the hand 
had rewards of 0. We used batch sizes of 250 GameExperiences, and played/trained
for ~1000 loops, for a total of 250,000 Game Experiences used to train each 
iteration of the model.

### State Representation
The main challenge of this project was state representation. We considered a few
ways to represent the cards (ignoring the other information for now):
1. 104-length vector (52 hole cards, 52 common cards) with 1s representing present
cards and 0s representing absent cards.
2. 27-length vector (13 for hole cards, 13 for common cards, 1 suit node) where
each node is a number from 0-4 representing how many cards of each rank are present.
The suit node is a number from 1-5 describing how close to a Flush a player is.
3. 16-length vector generated by an Autoencoder, which takes in a 52-length vector and
reduces the information to 8 real numbers. Using the Autoencoder for both hole
and common cards leads to 16 numbers.
4. 1 real number (scaled from -1 to 1) reflecting the Player's hand strength
(-1 = Junk, 1 = Royal Flush).

Additional information we including in the input to the Model was round (pre- or
post-flop), pot amount, and outstanding bet (amount to stay in the game).

### Results
We were able to create one semi-sensible model, but for the most part the models
exposed flaws in their opponents' strategies. For example, many variations of the 
model figured out that by constantly raising, they could induce a fold from our 
opponent, which was choosing actions probabilistically based on hand strength. 
We saw very little checking/folding after the model was trained on hundreds of 
thousands of GameExperiences, which may have been due to bias in the results from
early actions the model was taking. Overall, with a lot more testing and training
we think the model would be able to achieve better results.

### Future Questions
This project has stirred up a lot of interesting potential research questions 
such as:
1. What is the best way to represent a hand of cards and a Poker game state as 
input for a Neural Network?
2. How can we inject the idea of risk aversity into a model?
3. Is Poker a game that can benefit from RL, or is it too random, not sufficiently
action-dependent, and too reliant on hidden information?
4. Could an RL agent beat professional Poker players? Would there need to be 
randomness involved in choosing actions if so (rather than just selecting the
highest Q value at a certain state)?
5. What is the ideal model architecture, loss function, and quantity of training
data for an RL model like this one?
6. Would a model be able to learn a player's strategies/play style and exploit
their mistakes? How much play time would that require?

### Repository Contents
**pokerGame.py:** the main Poker file. Contains the game structure and runs
play/train loops during which batches of games are played and then used to 
train the model

**gameState.py:** the state of a Poker game. Contains information about players,
cards, bets, etc. 

**gameExperience.py:** an (s,a,s',r) learning experience

**model.py:** a Keras RL model that uses GameExperiences to learn optimal actions

**player.py:** a Poker player that uses the Model to choose actions

**smartPlayer.py:** a Poker player that chooses actions probabilistically based
on simple hand strength

**deck.py:** a collection of 52 cards

**card.py:** a Card object with suit and rank properties

**cardAutoencoder.py:** a Keras autoencoder that reduces a 52-length array of 
1s and 0s (representing a deck) to 8 numerical outputs

**constants.py:** model training and general game constants

**hand.py:** a list of cards in a player's hand

**handRankUtil.py:** functions that evaluate the in-game strength of a Poker hand

**handScoreUtil.py:** a function that compares a Player's hand with all other 
possible hands and determines the probability of winning

**model_tester.py:** input examples to test the model's performance while training

**utils.py:** utility functions for reading from and writing to files

**/output:** log files for iterations of the model run on Bowdoin's HPC cluster

**experiences.json:** a json list containing the most recent GameExperience information
(to pick from when training the model)

**run_autoencoder.sh:** a Bash script to train the autoencoder on the HPC cluster

**run_model.sh:** a Bash script to train the model on the HPC cluster

**ALL OTHER DIRECTORIES:** contains various iterations of trained models
